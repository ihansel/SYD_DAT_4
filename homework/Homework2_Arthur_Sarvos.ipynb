{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "## Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account (Done)\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "\n",
    "> Linear Regression, in its simplest form, tries to establish the linear relationship between an input (which we call the independent variable) and an output (which we call the dependent variable). Often we have multiple inputs which we feel influence the value of the output in a linear fashion. The p-value allows us to assess how important particular variables are in the overall prediction. A p-value that is small, say below 0.05, may suggest that the relationship between a particular proposed input is significant in explaining movements in the output. If the p-value is converserly greater than this threshold, we may conclude that their is not enough evidence to suggest a relationship between the input and output. The R-squared is a number between 0 and 1 which indicates the amount of variation in the output which has been successfully explained by the model we have chosen. A value closer to 1 is indicative of a good model. For example, a value of 0.75 means that 75% of the variation in the output is explained by the variation in the value of the inputs we have chosen.\n",
    "\n",
    "> In order to fit a linear regression model, we need to minimise the difference between a modelled value of the output, and the value of the output itself. Now given that our model may be biased in either direction over the course of the outputs we have observed, we need to minimise the squared difference. This will ensure that negative deferences do not cancel out with positive differences.  \n",
    "\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper?\n",
    "    > We've spoken about \n",
    "    > * K-nearest neighbours: This is essentially a method that uses the k most similar training examples to predict the label of a test example, usually by way of a majority vote. \n",
    "    > * Cross validation: This is a very powerful method that goes a long way in ensuring that a particular learning method does not overfit to the data. The method involves splitting the training data into k random folds for which the objective function is optimised. For instance, 5 fold cross validation will split the training data into 5 parts, using 4 of them to fit the initial model, and the 5th to use for hold out sample testing. The results are then averaged to attempt to come up with a model which generalises better to the test data.\n",
    "    > * The use of a scoreing function (or objective function). So we man be trying to maximise a likelihood function, or equivalently minimising a cost function of some kind.\n",
    "    > * We've breifly spoken about ways to get there. So for instance, using iterative techniques such as greedy search algorithms and gradient descent.\n",
    "    > * The notion of overfitting to the data and the connection to the bias variance trade-off. We've spoken about this above with respect to using cross-validation to help mitigate the effect of overfitting. However sometimes this is not enough.\n",
    "    >  * The use of regularisation in the evaluation of the objective function can help with this problem. We will encounter the concept of skrinkage soon, but methods of regularisation like LASSO we have already covered.\n",
    "    \n",
    "    * Explain sections 6-13 in your own words\n",
    "    > Section 6 refers the the difficulties faced in the machine learning field when dealing with higher dimensional problems. Bellman coined this term as the \"curse of dimensionality\". The trouble is twofold. Firstly, learning algorithms often do not generalise well to higher dimensions. Secondly, higher dimensional problems often requires a much very large training dataset due to the feature space the is then defined by the problem. Often this is not the case and the training sample will cover a very small region of the input space define by the problem. At the same time, it may not even be the correct feature space we want to look at in order to solve the problem at hand. The curse of dimensionality extends beyond this though. People can quite readily understand lower dimensional problems and the decision boundaries associated with learning algorithms. However, beyond 3 dimensions, it's hard to understand what is actually happening. One glimmer of hope lies in the fact that in a lot of applications, the data is not uniformally spead throughout the various dimensions, but are located on or near a lower dimensional manifold. A manifold is essentially a space that locally resembles that of a euclidean space, but in actual fact could involve many more dimensions. For instance a sphere is an example of a 2d manifold (ie. the world is flat!). The local region of someone on the sphere would be a plane.\n",
    "    \n",
    "    > Section 7 refers to the theorectical guarantees in the machine learning process with respect to learning the underlying signal. If the training set is large enough, in a probablistic sense, our learner has a greater likelihood of outputting a good classifier. One may look at the asymptotics, but these will often suggest results which we cannot begin to practically analyse. For instance, if one learner is better than another asymptotically, then buy implcation, the second is better on a given finite set of data. As the paper suggests, the main role of theoretcial guarantees is to understanding the driving forces of algorthmic design.\n",
    "    \n",
    "    > Section 8 stresses the importance of feature engineering. Often a dataset will not have a feature set that in their own right, correlate will with the output. In many cases, features will need to be developed from existing ones to help model the complex relationships that maty exist. Feature engineering is often a much harder problem as it is domain specific. It relies on understanding of the data in great detail. Learning algorithms are arguably easier sinse they are general. This also ties in well with the fact that machine learning cannot be simply a computer driven task. The symbosis between human and computer is apparent here. \n",
    "    \n",
    "    > Section 9 drills home one of the fundamentals of statistics. More data will often (if not always) produce better results. The choice between a better learning algorithm  or gathering more data should in large, be focused on the latter if possible. The 3 main limitations in machine learning are time, memory and training data. Given that these days we often have lots and lots of data we can use to learn an outcome, time is a key factor preventing complex functions to be learned. This leads to the reduction of complexity of the learning problem and thus less adequate predictions. A good takaway from this section is that simpler algorithms should be considered first. This is because the use of more complex techniques often adds to the layer of complexity of the fitting process. For instance, the use of hyperparameters mean that there are potentially a lot of dials that need to be considering in fitting a complex learner.\n",
    "    \n",
    "    > Section 10 suggests that people involved in machine should learn a wide variety of technqiues. Different problems will often yield learners producing stronger or weaker results. So each application may differ with respect to the best method that should be used. Ensembling has also proved very powerful, as illustrated by the Netflix prize winners, were teams generated ensembles combining over 100 learners.\n",
    "    \n",
    "    > Section 11 argues that simplicity does not imply accuracy. So take two classification models where the training error between the two are the same, but one is more complex than the other. This does not mean that the model with the simpler form will generalise better to the test data. Indeed the example of ensembling can illustrate how a much more complex model leads to a lower erro rate in the test data (where we may have 0% prediction error in two models in the training data). This often occurs in gradient boosting, where the test error will continue to drop with more trees even though the training error may have been zero earlier in the tree growing process. Which indicates that there is something quite powerful going on with the way these algorithms traverse the input space. \n",
    "    \n",
    "    > Section 12 touches on the disctinction between respesenting a function using a particular learner, and learning the function itself. The proposed learner algorithm way learn a function arbitrarily well **given** the data. However it does not mean the proposed method will actually learn the function. Given the sample data, time and computing resources, it is suggested to use a variety of learners, and even combine them if possible.\n",
    "    \n",
    "    > Section 13 reiterates the old saying that \"correlation does not imply causation\". Often learning algorithms are applied to observational data and thus are very good at highlighing correlations. Causation is a much more difficult question to answer and understanding the events which lead to other events is extremely difficult. Indeed a procedure to automatically discover this in an observational study (as opposed to experimental) remains a holy grail.  \n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning (Done)\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "\n",
    "> Subset selection: This process involves selecting a sample of predictors for the p available for which the analyst believes are related to the response variable. A method such as least squares is then used to fit the parameters of the proposed model to the data. The subset selection may be conducted via a forward stagewise approach, whereby predictors are added and the fit reassessed. Usually other methods of comparing the fit are used, such as adjusted R-squared, AIC and BIC. These methods penalise the model for a greater use of predictors. Another method may involve backwards selection. There are also regression techniques which perform both forward and backwards selection. \n",
    "\n",
    "> Shrinkage: This is a term refering to the regularisation of inputs during the fitting process. So in comparison to the first method, we include all p predictors into the model and perform techniques which reduce the variance of a particular objective function (say the least squares cost function). The effect of this may reduce particular predictors coefficients to zero. This then provides a method of automatic feature selection. Popular methods involve the use of lasso and ridge regression, which minimise the objective function with respect to the contraint determined by the L1 and L2 norms respectively.\n",
    "\n",
    "> Dimensionality Reduction: This method involves projecting the original p predictors onto an m dimensional subspace, where M < p. A way of achieving this is by eiganvalue decomposition. Ranking the correspondonding eiganvectors with respect to their variance allows us to determine which dimensions are suitable candidates for projecting onto that result in the minimal loss of information. \n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Complete the first 3 exercises from Chapter 3 in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: \n",
    "Describe the null hypotheses to which the p-values given in Table 3.4\n",
    "correspond. Explain what conclusions you can draw based on these\n",
    "p-values. Your explanation should be phrased in terms of sales, TV,\n",
    "radio, and newspaper, rather than in terms of the coefficients of the\n",
    "linear model.\n",
    "\n",
    "> The p-values here are indicating the probability that the given explanitory variable is significant in explaining the level of product sales. Under the null hypothesis, the level of TV, radio and newspaper individual have no impact on the level of sales. Given the p-value for TV and Radio is essentially zero, we can conclude that there is enough evidence to suggest that they play an important role in predicting the level of product sales. As we can also see, the significance of the intercept term means that product sales will occur even when advertising using these three methods are not used. It also appears that the level of product sales is not influenced by the level of newspaper advertising. This can be seen in the insignificant p-value of 0.8599. \n",
    "\n",
    "> With respect to change in value, both the significant factors for product sales (TV and Radio) influence in a positive direction over the data ranges used. One should question the use of this prediction method in extrapolating to other levels outside of the predetermined range as the relationships may (and often do) break down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "Carefully explain the differences between the KNN classifier and KNN\n",
    "regression methods.\n",
    "\n",
    "> The K-Nearest Neighbors classifier is used when your prediction problem is a classification problem (obvious!). The way the method works is that, given a test observation, the KNN classifier will identify the K points in the training data that were closest to this observation. Once these K points are identified, it will apply a majority vote. So if there are 5 out of 7 neighours classified to 1, then the test observation will be classified to 1. \n",
    "\n",
    "> The K-Nearest Neighbor regression problem is similar, but is used in the regression setting (i.e. we dont have classifications for our target. Our data is more than likely continuous). In the KNN regression case, the algorithm still identifies the K closest observations of a test observation to that in the training data. Instead of a majority vote type method, the average of all the training responses then forms the value of the estimate of the test observations target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "Suppose we have a data set with five predictors, X1 =GPA, X2 = IQ,\n",
    "X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between\n",
    "GPA and IQ, and X5 = Interaction between GPA and Gender. The\n",
    "response is starting salary after graduation (in thousands of dollars).\n",
    "Suppose we use least squares to fit the model, and get ˆβ0 = 50, ˆβ1 =\n",
    "20, ˆβ2 = 0.07, ˆβ3 = 35, ˆβ4 = 0.01, ˆβ5 = −10.\n",
    "\n",
    "\n",
    "(a) Which answer is correct, and why?\n",
    "> i. For a fixed value of IQ and GPA, males earn more on average\n",
    "than females. (Incorrect)\n",
    "\n",
    "> ii. For a fixed value of IQ and GPA, females earn more on\n",
    "average than males. (Correct)\n",
    "\n",
    " >> If we examine the coefficient for gender, it is 35. x3 = 1 implies a females will earn more on average, all else remaining constant. We also need to consider the interaction term however. This is X5. This is -10. which means that for a given GPA and IQ, females will on average earn more than males as the cumulative impact of both predictors, keeping all else constant, is positive.\n",
    "\n",
    "> iii. For a fixed value of IQ and GPA, males earn more on average\n",
    "than females provided that the GPA is high enough. (Correct) \n",
    "\n",
    "> iv. For a fixed value of IQ and GPA, females earn more on\n",
    "average than males provided that the GPA is high enough. (Incorrect) \n",
    "\n",
    ">> As we can see with the interaction term X5, if a females GPA is high enough, this will begin to cancel out the effect of the X3 term. resulting in a higher average for males provided their GPA is high enough. Note that GPA on it's own has a positive coefficient and thus increases the salary (which intuitively makes sense).\n",
    "\n",
    "> (b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.\n",
    "\n",
    "    Salary(female,IQ = 110,GPA = 4.0) =  50 + (20 * 4) + (0.07 * 110) + (1 * 35) + (0.01 * 110 * 4) - (10 * 1 * 4)\n",
    "        = 137.1 (thousand dollars)\n",
    "\n",
    "> (c) True or false: Since the coefficient for the GPA/IQ interaction\n",
    "term is very small, there is very little evidence of an interaction\n",
    "effect. Justify your answer.\n",
    "\n",
    ">>  This is False. Even if the value of the predictor is very small, we need to examine the p-value of the regression coefficient to determine if the interaction term (B4) is statstically significant or not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack. \n",
    "> I've created a new repository (https://github.com/atsarvos/Artys_Project_Data_Science.git) ..\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code) \n",
    "> Refer to my github repo .. I've done some prelim work on the business understanding and also the data understanding. \n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "> Currently I've not pregressed with the visualisation. Due to the missing data. I wanted to spend a bit more time cleaning the data before I progress the visualisations work.\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "> The target variable is a 0/1 indicator. So there are a lot of methods I could use on this dataset. The readme file discusses some of my potential options. I havent put the scikit learn functions here as I'm yet to investigate their full features. To be honest i think writing them down here explicitly is a bit pointless for now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
