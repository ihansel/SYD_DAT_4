{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "\n",
    "    ✓ Done\n",
    " \n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "\n",
    "Linear regression is used to model the relationship between one or more (independent) variables to another (dependant) variable. For example, to model the weight to height ratio of people.\n",
    "\n",
    "This can be visualised using Cartesian geometry.  In the case where there is a single independent variable, known data points can be plotted in two dimensions with the independent variable on the X axis and the dependant variable on the Y axis.  A straight line is then found that minimises the absolute distances between the data points and the line.\n",
    "This regression line is then used as an approximation of the relationship and for prediction. This can also be extended into three (or more) dimensions where three (or more) independent variables are used.\n",
    "\n",
    "There are various metrics used to evaluate the relationship as well as how well the line fits the data. To find the best line through the known data points, instead of using absolute distance of each point to the line, the distance is squared. This has the same effect of removing the problem of negative and positive distances cancelling each other. The best fit line is found that has the minimum sum of squared distances for the data points and the regression line.\n",
    "\n",
    "The slope of a regression line shows how related the dependant variable Y is to the independent variable X. A slope of 0 indicates no relationship and a slope of 1 indicates a total relationship.  However, as we use a sample to estimate the regression line of a population, we need to see if the slope value is significantly different from 0.  We can do this by calculating the standard error for the slope of the line and making sure that our slope value is more than 2 standard deviations from 0. This probability is the so called p-value which should be as small as required for the level of certainty. Usually this is 95% and thus a p-value of 0.05 or smaller.  \n",
    "\n",
    "The R<sup>2</sup> statistic is a measurement of how well a linear regression model fits the data. A R<sup>2</sup> value is a proportion (between 0 and 1) of how much of the variability of the output (Y) variable is explained by the input (X) variable. The closer it is to 1, the better the fit. \n",
    "R<sup>2</sup> is calculated using as: 1 – TSS / RSS where TSS is the total sum of squares (using differences of Y values to the mean of Y values) and RSS is the residual sum of squares (using differences of Y values to the estimated Y values using the regression line).\n",
    "\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "\n",
    "\n",
    "    * What have we covered so far from this paper? \n",
    "    \n",
    "        * machine learning\n",
    "        * k-nearest neighbours\n",
    "        * logistic regression\n",
    "        * overfitting\n",
    "        * dimensionality\n",
    "        * bias\n",
    "        * variance\n",
    "        * linear regression\n",
    "        * classification\n",
    "        * feature engineering\n",
    "        * recommendation\n",
    "                       \n",
    "    * Explain sections 6-13 in your own words\n",
    "    \n",
    "<em>Section 6. Intuition Fails In High Dimensions</em><br/>\n",
    "After overfitting, the biggest problem in machine learning is <em>the curse of dimensionality</em>. Many algorythms that work well in low dimensions become intractable when the input has many dimensions. This is because a fixed size example data set covers an exponentially smaller space as dimensionality increases.\n",
    "An example of this is in the K nearest neighbours method, where in high dimensions cause the distances between the data to be similar and where noise in the data swamps the real signals. Another problem occurs in high dimensions as the shape of data is difficult to approximate.\n",
    "This problem is sometimes reduced as data is often clustered and not spread out uniformely thus effectively reducing the problem space.\n",
    "\n",
    "<em>Section 7. Theoretical Guarantees Are Not What They Seem</em><br/>\n",
    "Machine learning papers are full of theoretical guarantees but we should be careful about their application and what they actually mean.\n",
    "We need to be careful about the size of our training set. Some guarantees assume infinite data (asymptotic) which never actually holds.\n",
    "The main role of theoretical guarantees are as a way of understanding and comparing different algorythms.\n",
    "\n",
    "<em>Section 8. Feature Engineering Is The Key</em><br/>\n",
    "One of the most important factor of successful machine engineering projects is the selection of the data features used. \n",
    "It pays to spend time to choose and also engineer the best possible features. This may involve much investigation trial and error. \n",
    "This also involves domain expertese and an iterative process.\n",
    "\n",
    "<em>Section 9. More Data Beats Clever Algorithm</em><br/>\n",
    "\n",
    "There are two ways to improve a machine learning effort, design or choose a better algorithm or collect more data. The easiest way may be to get more data - more examples and/or more features. A simple algorithm with lots of data often beats a complex algorithm with little data.\n",
    "\n",
    "In machine learning there is a scalability problem with limited resources of time, memory and data. These days, the bottleneck is often time. There is not enough time to process using complex classifiers and simplier ones are used, which often produce good results. It pays to use the simplest learners first and progress to more complex and harder to use methods if necessary.\n",
    "\n",
    "The human component of a project may be the biggest cost or bottleneck. Learning algorithms are typically measured in terms accuracy and computational cost.  But human effort may be a bigger factor.  This highlights the importance of simplicity first, using learners that are more human understandable, infrastructures that facilitate experimenting with different learners and the collaboration of data scientists with domain experts.\n",
    "\n",
    "<em>Section 10. Learn Many Models, Not Just One</em><br/>\n",
    "The best learning algorithm to use depends on the application. It has also been found that often a better result can be obtained by combining several models (model ensembles). There are several techniques to combine models and the trend is toward larger ensembles.\n",
    "There is also a way of averaging several models called Bayesian model averaging but is not as useful as model ensembles.\n",
    "\n",
    "<em>Section 11. Simplicity Does Not Imply Accuracy</em><br/>\n",
    "Although it is often best to choose the simpliest model that fits the training data, it may not produce the most accurate results. Choosing another more complicated or an ensemble may be necessary.  Simplicity should be preferred but while easier to understand there may be a trade off with less accuracy.\n",
    "\n",
    "<em>Section 12. Representable Does Not Imply Learnable</em><br/>\n",
    "Just because a function can be represented does not mean it can be learned. For various reasons a learning method may not work well in a particular application. For example, there may be a local optimum chosen by a greedy algorithm. It is important to try out different learners and possibly combine them.\n",
    "Different algorithms also have vastly different resourse requirements.\n",
    "\n",
    "<em>Section 13. Correlation Does Not Imply Causation</em><br/>\n",
    "It is easy to fall into the trap of assuming correlation implies causation.  These machining learning algorithms can not show causation - probably need to do a well designed randomized controlled experiment for that. However, the main goal of a machine learning project is to produce actionable results and it may be difficult to show cause for recommended actions without experimentation. \n",
    "    \n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning\n",
    "\n",
    "    ✓ read 120 pages of text book\n",
    "\n",
    "\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "\n",
    "    From http://machinelearningmastery.com/an-introduction-to-feature-selection/ ...<br/>\n",
    "    There are three general types of feature selection:\n",
    "    1. Filter methods: Chi squared test, information gain and correlation coefficient scores.        \n",
    "    2. Wrapper methods: recursive feature elimination algorithm. \n",
    "    3. Embedded methods: LASSO, Elastic Net and Ridge Regression.\n",
    "\n",
    "\n",
    "* Complete the first 3 exercises from Chapter 3 in Python\n",
    "\n",
    "     ✓ <b>see below</b>\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "    <b>Not done yet. I am still deciding on project - this weekend hopefully.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ISL Chapter 3 Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                       0.897\n",
      "Model:                            OLS   Adj. R-squared:                  0.896\n",
      "Method:                 Least Squares   F-statistic:                     570.3\n",
      "Date:                Fri, 29 Apr 2016   Prob (F-statistic):           1.58e-96\n",
      "Time:                        11:49:25   Log-Likelihood:                -386.18\n",
      "No. Observations:                 200   AIC:                             780.4\n",
      "Df Residuals:                     196   BIC:                             793.6\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.9389      0.312      9.422      0.000         2.324     3.554\n",
      "TV             0.0458      0.001     32.809      0.000         0.043     0.049\n",
      "Radio          0.1885      0.009     21.893      0.000         0.172     0.206\n",
      "Newspaper     -0.0010      0.006     -0.177      0.860        -0.013     0.011\n",
      "==============================================================================\n",
      "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
      "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
      "Kurtosis:                       6.332   Cond. No.                         454.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "df = pd.read_csv(\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\")\n",
    "\n",
    "fit = sm.ols(formula=\"Sales ~ TV + Radio + Newspaper\", data=df).fit()\n",
    "print(fit.summary())\n",
    "#print(fit.pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 (a) Describe the null hypotheses to which the p-values given in Table 3.4 correspond. \n",
    "\n",
    "The null hypothesis is that there is no relationship of the input variables (TV, Radio and Newspaper) to the output variable (Sales).\n",
    "\n",
    "We have the linear equation:\n",
    "    \n",
    "    sales = β0 + β1 * TV + β2 * radio + β3 * newspaper + ε\n",
    "\n",
    "and having fitted the linear model we get:\n",
    "\n",
    "    β0 =  2.9389\n",
    "    β1 =  0.0458\n",
    "    β2 =  0.1885\n",
    "    β3 = -0.0010\n",
    "    sales = 2.9389 + 0.0458 * TV + 0.1885 * radio - 0.0010 * newspaper + ε\n",
    "\n",
    "The null hypotheses is that:\n",
    "\n",
    "    β1 = β2 = β3 = 0\n",
    "\n",
    "We need to show that these coefficients (fitted for the sample data) are significantly different than zero.\n",
    "The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect).\n",
    "A low p-value (< 0.05) indicates that we can reject the null hypothesis.\n",
    "\n",
    "\n",
    "#### 1 b) Explain what conclusions you can draw based on these p-values. \n",
    "Since we get low p-values for TV and Radio we reject the null hypothesis for these (i.e. we accept that they relate significantly to the sales).\n",
    "\n",
    "Since we get a high p-value for Newspaper we can not reject the null hypothesis (that there is no relationship for Newspaper advertising).\n",
    "\n",
    "Based on this, we should probably drop the Newspaper advertising variable from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISL Chapter 3 Exercise 2. Carefully explain the differences between the KNN classifier and KNN regression methods.\n",
    "\n",
    "The KNN classifier is used to determine which class an observation belongs to. An object is classified by a majority vote of its K nearest neighbours.\n",
    "\n",
    "The KNN regression method is used to estimate output quantities from observations.  This output value is the average of the values of its K nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISL Chapter 3 Exercise 3. \n",
    "\n",
    "Suppose we have a data set with five predictors, \n",
    "\n",
    "    X1 = GPA\n",
    "    X2 = IQ \n",
    "    X3 = Gender (1 for Female and 0 for Male)\n",
    "    X4 = Interaction between GPA and IQ\n",
    "    X5 = Interaction between GPA and Gender\n",
    "    \n",
    "The response is starting salary after graduation (in thousands of dollars). \n",
    "\n",
    "Suppose we use least squares to fit the model, and get \n",
    "\n",
    "    β0 = 50 \n",
    "    β1 = 20 \n",
    "    β2 = 0.07\n",
    "    β3 = 35\n",
    "    β4 = 0.01\n",
    "    β5 = −10\n",
    "\n",
    "We have the model:\n",
    "\n",
    "    Salary  = 50 + 20*GPA + .07*IQ + 35*Gender + .01*GPA*IQ - 10*GPA*Gender\n",
    "    \n",
    "    \n",
    "For females (Gender = 1) this becomes:    \n",
    "\n",
    "    SalaryF = 50 + 20*GPA + .07*IQ + .01*GPA*IQ + 35 - 10*GPA\n",
    "\n",
    "For males (Gender = 0) this becomes:    \n",
    "\n",
    "    SalaryM = 50 + 20*GPA + .07*IQ + .01*GPA*IQ\n",
    "\n",
    "The difference between female and male salary is:\n",
    "\n",
    "    35 - 10*GPA\n",
    "    \n",
    "This means at low GPA females earn more, at high GPA males earn more.\n",
    "\n",
    "#### (a) iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary for a female with GPA 4.0 and IQ 110 is $137100.00\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.\n",
    "'''\n",
    "\n",
    "MALE = 0; FEMALE = 1;\n",
    "\n",
    "def salary(gpa, iq, gender):\n",
    "    return 50 + 20*gpa + .07*iq + 35*gender + .01*gpa*iq - 10*gpa*gender \n",
    "\n",
    "gpa = 4.0\n",
    "iq = 110  \n",
    "salaryInDollars = salary(gpa, iq, FEMALE) * 1000\n",
    "\n",
    "print(\"Salary for a female with GPA %s and IQ %s is $%.2f\" % (gpa, iq, salaryInDollars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n",
    "\n",
    "False, the evidence of the interaction effect is determined from the t-statistic (or p-value) and even though the term is small the variables have been scaled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
