{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle competition: Bag of Words Meets Bags of Popcorn\n",
    "\n",
    "Kaggle competition tutorials: https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
    "\n",
    "I have adapted this tutorial: http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/\n",
    "\n",
    "\n",
    "<b>Goal:</b> To experiment with text classification. Try out different models and vectorization.\n",
    "\n",
    "<b>Overview:</b>\n",
    "- use labeled data from this kaggle competition\n",
    "- split data into train and test sets\n",
    "- use different models (random forest, linear regression)\n",
    "- use different feature vectorizers (word count, TF-IDF)\n",
    "- evaluate using AUC\n",
    "\n",
    "Note:\n",
    "TF-IDF stands for “term frequency / inverse document frequency” and is a method for emphasizing words that occur frequently in a given document, while at the same time de-emphasising words that occur frequently in many documents.\n",
    "\n",
    "<b>Results:</b> \n",
    "(see end of notebook):\n",
    "\n",
    "- random forest model is not ideal for high-dimensional sparce data\n",
    "- linear regression works better and faster\n",
    "- TD-IDF gives a big improvement keeping in stopwords and using n-grams (compared with simple word count vectorization)\n",
    "\n",
    "- better to remove stopwords when using random forest and word count\n",
    "- better to keep stopwords when using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to read data\n",
    "\n",
    "def read_data():\n",
    "    return pd.read_csv('data/labeledTrainData.tsv', header = 0, delimiter = \"\\t\", quoting = 3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions for cleaning reviews\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))                                                   \n",
    "\n",
    "def clean_include_stopwords(review):\n",
    "    text = BeautifulSoup(review, 'lxml').get_text() \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)       \n",
    "    words = text.lower().split()\n",
    "    return \" \".join(words)   \n",
    "\n",
    "def clean_exclude_stopwords(review):\n",
    "    text = BeautifulSoup(review, 'lxml').get_text() \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)       \n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if not word in stop_words]   \n",
    "    return \" \".join(words)   \n",
    "\n",
    "# Function to clean data \n",
    "def clean_data(data, cleaner):\n",
    "    data['review'] = data['review'].apply(cleaner)    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to split data into train and test sets\n",
    "\n",
    "def split_train_test(data):\n",
    "    train_i, test_i = train_test_split(np.arange(len(data)), train_size = 0.8, random_state = 44)\n",
    "    train = data.ix[train_i]\n",
    "    test = data.ix[test_i]\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "data_include_stopwords = clean_data(read_data(), clean_include_stopwords)\n",
    "data_exclude_stopwords = clean_data(read_data(), clean_exclude_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to analyse model with vectorizator\n",
    "\n",
    "def analyse(vectorizer, model, remove_stopwords):\n",
    "    \n",
    "    data = data_exclude_stopwords if remove_stopwords else data_include_stopwords    \n",
    "    train, test = split_train_test(data)\n",
    "        \n",
    "    train_features = vectorizer[0].fit_transform(train['review']).toarray()    \n",
    "    model[0].fit(train_features, train[\"sentiment\"])\n",
    "    \n",
    "    test_features = vectorizer[0].transform(test['review']).toarray()    \n",
    "    predictions = model[0].predict_proba(test_features)\n",
    "    \n",
    "    return AUC(test['sentiment'].values, predictions[:,1] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make vectorizers\n",
    "\n",
    "def make_count_vectorizer(max_features):\n",
    "    return CountVectorizer(max_features=max_features, analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None)\n",
    "    \n",
    "def make_tfidf_vectorizer(max_features):\n",
    "    return TfidfVectorizer(max_features=max_features, ngram_range = (1, 3), sublinear_tf = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 5000 \tstopwords:  True \tauc:  0.916471261383\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 5000 \tstopwords:  False \tauc:  0.916682627098\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 10000 \tstopwords:  True \tauc:  0.920125840203\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 10000 \tstopwords:  False \tauc:  0.918946288308\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 20000 \tstopwords:  True \tauc:  0.924231071208\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 20000 \tstopwords:  False \tauc:  0.918999809755\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 30000 \tstopwords:  True \tauc:  0.925930397158\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 30000 \tstopwords:  False \tauc:  0.919006129926\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 40000 \tstopwords:  True \tauc:  0.925807353831\n",
      "model:  Random Forest \tvectorizer:  Count, \tmax features: 40000 \tstopwords:  False \tauc:  0.918865166114\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 5000 \tstopwords:  True \tauc:  0.920524810991\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 5000 \tstopwords:  False \tauc:  0.916282696284\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 10000 \tstopwords:  True \tauc:  0.9248298874\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 10000 \tstopwords:  False \tauc:  0.920363446628\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 20000 \tstopwords:  True \tauc:  0.926364488896\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 20000 \tstopwords:  False \tauc:  0.922979037353\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 30000 \tstopwords:  True \tauc:  0.928851036132\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 30000 \tstopwords:  False \tauc:  0.925187657074\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 40000 \tstopwords:  True \tauc:  0.92845894553\n",
      "model:  Random Forest \tvectorizer:  TFIDF, \tmax features: 40000 \tstopwords:  False \tauc:  0.924227311106\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 5000 \tstopwords:  True \tauc:  0.925535986493\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 5000 \tstopwords:  False \tauc:  0.928056454647\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 10000 \tstopwords:  True \tauc:  0.936397880199\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 10000 \tstopwords:  False \tauc:  0.937968802676\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 20000 \tstopwords:  True \tauc:  0.940526391834\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 20000 \tstopwords:  False \tauc:  0.942526605919\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 30000 \tstopwords:  True \tauc:  0.942061153334\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 30000 \tstopwords:  False \tauc:  0.944035526721\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 40000 \tstopwords:  True \tauc:  0.942542926361\n",
      "model:  Linear Regres \tvectorizer:  Count, \tmax features: 40000 \tstopwords:  False \tauc:  0.944544820492\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 5000 \tstopwords:  True \tauc:  0.95239311271\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 5000 \tstopwords:  False \tauc:  0.953702908127\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 10000 \tstopwords:  True \tauc:  0.95588424711\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 10000 \tstopwords:  False \tauc:  0.957914702014\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 20000 \tstopwords:  True \tauc:  0.957898861585\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 20000 \tstopwords:  False \tauc:  0.960345007729\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 30000 \tstopwords:  True \tauc:  0.958366394227\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 30000 \tstopwords:  False \tauc:  0.961210471131\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 40000 \tstopwords:  True \tauc:  0.958487837511\n",
      "model:  Linear Regres \tvectorizer:  TFIDF, \tmax features: 40000 \tstopwords:  False \tauc:  0.961770966287\n"
     ]
    }
   ],
   "source": [
    "# Run analysis of various models and vectorization\n",
    "# NOTE: THIS TAKES A LONG TIME TO RUN!\n",
    "\n",
    "def results(m, v, s, a):\n",
    "    print(\"model: \", m[1], \"\\tvectorizer: \", v[1], \"\\tstopwords: \", s, \"\\tauc: \", a)\n",
    "\n",
    "\n",
    "model_linear_regres = LR()\n",
    "model_random_forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "models = [(model_random_forest, \"Random Forest\", ), \n",
    "          (model_linear_regres, \"Linear Regres\", )]\n",
    "\n",
    "vectorizers = []\n",
    "max_features = [5000, 10000, 20000, 30000, 40000]\n",
    "for m in max_features:\n",
    "    vectorizers.append((make_count_vectorizer(m), \"Count, \\tmax features: \" + str(m)))\n",
    "for m in max_features:\n",
    "    vectorizers.append((make_tfidf_vectorizer(m), \"TFIDF, \\tmax features: \" + str(m)))\n",
    "\n",
    "for m in models:\n",
    "    for v in vectorizers:\n",
    "        for s in [True, False]:\n",
    "            results(m, v, s, analyse(v, m, s))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
