{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "    * Explain sections 6-13 in your own words\n",
    "\n",
    "\n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "* Complete the first 3 exercises from Chapter 3 in Python\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### What is Linear regression?\n",
    "Linear regression is a technique where given some known input variables, we try to make a model that predicts dependent, or output variables.\n",
    "Suppose we have a problem where we want to predict an unknown outcome or an output variable based on known input variables. A basic example is trying to predict the selling price of a house. In this example we might know some details about the house such as the size of the lot, the size of the floor space, the number of bedrooms and the number of bathrooms. To be able to make a linear regression model, we need to have some historical data of previous house selling prices and the information about each of these previous sales. With this historical data, we can make an equation that can predict the selling price, provided we also have enough information about each house. Linear regression will try to fit a line equation so that it works on the historical (training) data we have. \n",
    "\n",
    "How well a linear regression model fits can be determined by the p-values and R-squared value. A p-value indicates the likelihood that a term has no effect on the model, so if a p-value is low against a certain term in the linear regression equation, this indicates that this term is likely to be good to keep as a predictor in the model. Conversely, a high p-value means that the term is not likely to be a meaningful predictor in the regression model. The R-squared value is a number ranging between 0 and 1 that indicates how closely the training data \"fits\" the regression line. A R-squared value of 0 indicates that the models explains none of the variability of the output, whereas an R-squared value close to 1 indicates the models explains a lot of the variability of the output variable.\n",
    "\n",
    "> Excellent description and use of a simple example for reference. Easy to understand description - great work.\n",
    "\n",
    "##### What have we covered so far from the paper (Useful things to know about machine learning)? \n",
    "We've covered that the goal in making useful machine learning models is good generalization and that to achieve this it is important to keep in mind not to overfit the model to the training data. Good practices for machine learning are to separate test and training data and to cross-validate. \n",
    "\n",
    "Section 4 covers the importance of being apply to apply some domain-specific knowledge or assumptions to the data in order to shape the learning program beyond the data. This domain-specific knowledge can help by shaping the selection of dependences and by influencing data scientists in understanding the problem they are trying to solve, potentially aiding in dimensionality reduction.\n",
    "\n",
    "Section 5 goes over the problem of trying to create models that minimise both bias and variance; the goal should be to create models that fit both the test and training data. The section also mentions cross-validation and goes into more detail on regularization which introduces a penalty term to avoid introducing too many terms into the model.\n",
    "\n",
    "> We've covered sections 1-5. I think that's what you're summarising in your first paragraph, but if you need any connection from the work we've done to those topics let me know. \n",
    "\n",
    "##### Explain sections 6-13 in your own words\n",
    "The \"curse of dimensionality\" refers to how generalizing becomes exponenetially harder as the examples get more and more dimensions. Essentially as a data set gets more and more columns, it gets harder to generalize. With highly dimensional data, it gets much more difficult to apply similarity-based reasoning because the additional dimensions add a lot of noise when trying to calculate similarity between data points. For example, two data points might be classed as nearest neighbours based on 2 dimensions, but the signals from all the other dimensions in a highly dimensional dataset could potetnially swamp the similarity signal.\n",
    "\n",
    "> Nice description of what happens. \n",
    "\n",
    "Section 7 suggests caution when applying learnings machine learning papers with theoretical guarantees. As these guarantees apply in a threoretical environment, it would be advisable to consider how well the application environment aligns with the theoretical environment.\n",
    "\n",
    "> Yes, it's always important to keep in mind what youget in practice may not be what you're expecting from theory. So it's a good idea to check with cross-validation.\n",
    "\n",
    "Section 8 highlights the importance of domain-specific knowledge and the part it plays in influencing the features used in a model. This less technical side of data science is just as important as the technical side, and where most of the effort in a machine learning project goes into. A machine learning project is usually an iterative process, where the process of building, running and modifying the learner is repeated. The most useful learners are those that apply domain-specific knowledge. It is often a good idea to apply feature engineering as a first pass at dimensionality reduction.\n",
    "\n",
    "> It is a very iterative process. Getting the iteration cycles down to a minimum really helps improve the overall modelling process.\n",
    "\n",
    "Section 9 is titled \"More Data Beats a Clevered Algorithm\" however this is not always a viable option to take. There might not be more training data available to refine the learner, or there might not be enough time and resources to process additional raw training data. When dealing with highly-dimensional data, the \"curse of dimensionality' may mean no existing amount of data may be enough. It is for these reasons that clever algorithms are more likely to pay off, despite the greater effort required in refining them. It is useful for machine learning practitioners to have some expertise in learner design. Organistions that make data access and collaboration between machine learning and domain-specific experts easy and efficient stand the most to gain from machine learning.\n",
    "\n",
    "> Good point, the tricky thing is you don't want to be extrapolating from a small number of samples (relative to the inferences you are trying to draw out). That's why there is such a focus for pragmatic data scientists to find more data\n",
    "\n",
    "Section 10 describes the recent trend of making model ensembles, in which multiple variations of models are used in combination istead of selected the single best one. There are different ways of combining models. Examples given are *bagging*, where the results are combined by voting on the results of diffrent sample variations of the training set, *boosting*, where each new classifier focuses on examples where previous classifiers tended to get wrong and *stacking*, where outputs of classifiers are fed into a higher-level classifier. \n",
    "\n",
    "> Great explanation of combining models. When we can find complimentary model types then ensembling with them adds a lot of value to the modelling process.\n",
    "\n",
    "Section 11 aims to dispel the notion of simplicity of a learner model influencing the test error. A counter-example is the successful use of model ensembles in winning machine learning prizes for accuracy.\n",
    "\n",
    "> Yes, simple is a misleading word some times in machine learning. And XG-Boost is a great example of an ensemble method that performs excellently after many passes of the data.\n",
    "\n",
    "\"Correlation does not imply causation\" is a well known saying and it applies in the machine learning domain. Machine learning as discussed in the paper can only learn correlations. These correlations can then serve as a guide to conducting experiments and investigation, which can then provide causal information. While machine learning mainly deals in observational data, where predictive  variables are outside the observer's control, where possible it is often a good idea to obtain experimental data.\n",
    "\n",
    "> Causal inference is a very interesting area right now (look at Google's Causal Impact R package). Without thoughtfuly designed experiements it is very difficult to draw causal conclussions from some phenomena.\n",
    "\n",
    "##### Describe 3 ways we can select what features to use in a model\n",
    "\n",
    "Ridge regression is an approach that adds a term that penalises the addition of using too many predictiors with high coefficients in the regression model. This differs from the Lasso regression in that all coefficients will be non-zero. Resulting coefficients may approach zero but will not actually reach it.\n",
    "\n",
    "Lasso regression is an approach similar to ridge regression in that it also penalises too excessive significant coeffcients but differs in that resulting coefficients may reach zero.\n",
    "\n",
    "Principal Components Regression is an option that derives a low-dimensional set of features from a large number of variables and is applied in unsupervised learning. This technique carries out a transformation of the variables into principle components, each of which is a linear combination of the starting variables. The first principle components contributes to most of the variation, with each subsequent principal component contributing less and less to the variation.\n",
    "\n",
    "> Excellent description. \n",
    "\n",
    "##### Complete the first 3 exercises from Chapter 3 in Python\n",
    "\n",
    "**Exercise 1**\n",
    "\n",
    "The p-values here indicate the probability that each variable is *not* significant in explaining variability in the dependent variable sales. The low p-values for TV and Radio indicate that these two budgets are more influential in predicting sales and should be included in a regression model. The relatively high p-value for newspaper suggests that sales are not greatly influenced by the newspaper advertising budget. Interestingly, the low p-value for Intercept suggest that sales would occur even if budgets for TV, radio and newspaper were all zero.\n",
    "\n",
    "> Excellent interpretation. Very well put in terms of the original problems and what the p-values actually *mean* in practical terms. Well done!\n",
    "\n",
    "**Exercise 2**\n",
    "\n",
    "The KNN classifier is used in for classification problems. In this method, for a test observation, the K nearest points out of the training data set are identified. The classification that has the most number of these K neighbours is then assigned to the test data point.\n",
    "\n",
    "Where the KNN regression method differs is that instead of applying a classification to a test data point based on K nearest neighbours, an average of the dependent variable from these K nearest neighbours is assigned instead as the predicted response.\n",
    "\n",
    "> Correct.\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "a) \n",
    "i. For a fixed value of IQ and GPA, males earn more on average than females.\n",
    "\n",
    "Incorrect - Gender is encoded as 1 for female and 0 for male and the X3 coefficient is 35 while the X5 coefficent is -10. For a fixed value of IQ and GPA the combined effect of X3 and X5 is positive, meaning that for a fixed value of IQ and GPA females earn more on average than males.\n",
    "\n",
    "ii. For a fixed value of IQ and GPA, females earn more on average than males.\n",
    "\n",
    "Correct - as above\n",
    "\n",
    "> This is technically not correct. The only correct answer was iii (which you got). The reason for this is in the wording of the answer - in particular *females earn more on average than males*. We cannot really say this when GPA and Gender is a factor AND we don't know the underlying distribution of Female GPA scores. So we cannot say this is correct without more information.\n",
    "\n",
    "iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.\n",
    "\n",
    "Correct - If $\\beta_{5} \\times X_{5}$ (the GPA-Gender interaction) is low enough, then this starts outweighing the influence of $\\beta_{3} \\times X_{3}$\n",
    "\n",
    "iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.\n",
    "\n",
    "Incorrect - as above\n",
    "\n",
    "b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.1\n",
      "CORRECT\n"
     ]
    }
   ],
   "source": [
    "FemaleSalary = 50 + 4*20 + 0.07*110 + 1*35 + .01*110*4 - 10*1*4\n",
    "print FemaleSalary\n",
    "print \"CORRECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) False. The coefficient does not determine the validity of including a predictor in a regression model. The independent variable(s) that multiply with a small coefficent could be quite high and could still be a significant term in the regression model. The p-values should determine whether or not to include a variable as a predictor in a regression model.\n",
    "\n",
    "> CORRECT. Well put.\n",
    "\n",
    "#### Course Project\n",
    "\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "\n",
    "My Project github repository is here: https://github.com/dominictabeta/DAT4Project-DominicTabeta.git\n",
    "\n",
    "\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "\n",
    "I have carried this out in my github repo\n",
    "\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "\n",
    "I've yet to work on visualisations as I intend to first carry out some feature engineering to reduce the amount of features the learner will use\n",
    "\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "I would consider using logistic regression and KNN techniques in this project. I'll add more as I learn about them in class and in readings. The scikit learn function for logistic regression is LogisticRegression() and for KNN it's KNeighborsClassifier(). In addition to the previously mentioned techniques I would apply PCA function decomposition.PCA() for dimensionality reduction.\n",
    "\n",
    "> Great work Dominic, excellent homework submission and progress on the project! If any of the comments aren't clear or you want more elaboration please let me know.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
