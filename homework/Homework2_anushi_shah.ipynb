{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account - *Done*\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "\n",
    "    **Answer**\n",
    "    \n",
    "    * To explain what linear regression is, let me give you an example. As you know Airbnb is a popular website where you can rent your house/room for short term. Sydney is a popular tourist desitnation and it's a great way to earn extra money by renting a spare room/house for few days/weeks to visitors. However, your question might be how do estimate a suitable rent for my house. The answer is that your house rent depends of various factors like the size(sq feet), number of rooms, number of bathrooms, location, etc. You can find similar houses rented on ABB and estimate your own house rent. Of course, manually matching your house with thousands of other similar houses is a tedious and lengthy process, which may not always fetch you an accurate answer. This is where linear regression can help. \n",
    "    \n",
    "    * Linear regression is machine learning method where predict value y(rent) which is a dependent variable using some historic ABB rental data having X independent features(size(sq feet), number of rooms, number of bathrooms, location). It is a supervised machine learning method as our data is labelled with rents of other houses as well. It is called regression as our target variable(rent) is numeric and we are predicting continuous number.\n",
    "    \n",
    "    * *Linear regression* model fits a linear function(straight line) to observed data points x to obtain y.\n",
    "\n",
    "    * Linear function is y = X$\\beta$ + $\\epsilon$ where y is target variable, X is input variable, $\\beta$ is co-efficients and $\\epsilon$ is error term.\n",
    "\n",
    "    * In case of one input variable(e.g.size(sq feet)) , process is called *simple linear regression* and in case of more than one input variable(size(sq feet), number of rooms, number of bathrooms, location), it is called *multiple linear regression*\n",
    "    \n",
    "    * Before we predict our house rent, we need to be sure if house rent really depends on size(sq feet), number of rooms, number of bathrooms, location. Let's be sceptical and hypothesize that there's no relation between y(rent) and X(our features). We run our data through a hyposthesis test and get p-value. If p-value is too low (< 0.05), then we reject our hypothesis. But if p-value is high (> 0.05) then our hypothesis is correct. \n",
    "    \n",
    "    * Once we fit our Linear regression model, we need to find how good our model is. This can be achieved by R-squared which shows how close our data is fitted with regression line.Value of R-squared is between -1 and 1. -1 means there is negative linear relationship and +1 means there is positive linear relationship. \n",
    "\n",
    "\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "    \n",
    "    **Answer**\n",
    "    \n",
    "    So far we have covered folowing in the class :- classification(logistic regression), generalization goal of ML methods, overfitting, training/test datasets, bias, variance, cross validation and regularization, dimensionality reduction.\n",
    "    \n",
    "    * Explain sections 6-13 in your own words\n",
    "    \n",
    "    **Answer**\n",
    "    \n",
    "    * Intuitions fail in high dimensions - Curse of dimensionality is one of biggest problems of machine learning which in terms of ML means that it becomes harder to generalize correctly as the number of features(dimensionality) increases. This is because training set is small fraction of input space. Similarity based reasoning that ML methods like nearest neighbors are based on also fails in high dimensions. There is a partial solution to this problem called \"blessing of non-uniformity\" which means that in most applications, examples are spread on or near low dimension manifold which learners can take advantage of.\n",
    "     \n",
    "    * Theoretical gurantees are not what they seem - Bounds on the number of examples needed to ensure good generalization is one of the theoretical guarantees mentioned in many ML papers. However, recent research shows that in practice such methods may not work.\n",
    "      \n",
    "    * Feature engineering is the key - One of the most difficult tasks of ML is to select appropriate features. If the features are independent and and co-relate well then it's easy. But if class is a very complex function of features, then it's diffcult. Quite often raw data is not directly fed into ML algorithms, they are preprocessed which is a time consuming process as well. This entire process is iterative. Also feature engineering is requires good domain knowledge. These days feature engineering is being automated to select best candidate features.\n",
    "    \n",
    "    * More data beats a cleverer algorithm - After feature selection, to improve the accuracy of the classifier, a general rule of thumb is to use simple algorithm with lots of data than a complex and clever algorithm with modest amounts of it. This is because complex algorithms takes more time to run which means they are not scalable.\n",
    "    \n",
    "    * Learn many models, not just one - In earlier days, experts had favourite algorithms and would use variations on it to improve accuracy. Today model ensemble is pretty common where experts combine their algorithms to create a best one. Bagging and boosting are common ways of model ensembling. \n",
    "    \n",
    "    * Simplicity does not imply accuracy - It is generally believed that if there are classifiers with same training error then the simpler one will have likely have low test error. But there are many counter intuitive examples as seen by model ensembles, support vector machines(SVM), sign(sin(ax)) algorithms.\n",
    "    \n",
    "    * Representable does not imply learnable - Functions have associated theorems with them and hence they can be representable. However, not all of them can be learnable. e.g. - standard decision trees cannot learn beyond training set. So it's a good idea to try many leaners.\n",
    "    \n",
    "    * Correlations does not imply causations - It is pretty well known that correlations don't necessarily imply causations. Predictive analytics use them as reference to further action. For example, if we find that beer and diapers are often bought together then they could be put together to increase their sales. Since ML is applied on observational data and not experimental data, it may not be able to find causation. While it may not be necessary to know that and correlations can be used for further investigations.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning - *Done*\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "\n",
    "  **Answer**\n",
    "  \n",
    "  There are 3 ways in which features can be selected to be used in a model :\n",
    "  \n",
    "  (i) **Subset Selection** : In this approach, we select a subset of p features that we believe are related and then fit the model using least squares on the selected subset of p features. There are three ways to of this approach. \n",
    "   \n",
    "   First is **Best Subset Selection** where the model is fitted using all possible combinations of p features(i.e. 2<sup>p</sup> models) and finding the best one using using crossvalidated prediction error, Cp (AIC), BIC, or adjusted R<sup>2</sup>. Although it is a simple method, it becomes computationally infeasible for large values of p. It may work well on training data but may be not on test/real data.\n",
    "   \n",
    "   Second is **Stepwise Selection** where search space is smaller as compared to Subset Selection. **Forward Stepwise Selection** starts with no model and then adds one predictor at a time until all predictors are added. It has 1+p(p+1)/2 models, which makes it computationally feasible. It can be used when n < p. **Backward Stepwise selection** starts with all p features and removes least useful feature in each iteration and hence it has 1+p(p+1)/2 models. It can be used when n > p. \n",
    "\n",
    "   Third is **Hybrid Approaches** use a combination of Forward and Backward Stepwise where it adds a feature in each step, but also removes the one which is not useful.\n",
    "    \n",
    "  (ii) **Shrinkage** : In this approach, all p features are used but their coefficient estimates are shrinked towards 0. A tuning parameter alpha imposes penalty on the size of coefficients.By increasing the alpha, it penalizes the coefficients and shrinks them toward 0. \n",
    "  \n",
    "  **Ridge Regression** is similar to least squares, except we include a penalty term. When it is zero we get least squares,as it increases the term, the shrinkage penalty has more of an impact and the coefficients will approach 0. \n",
    "  \n",
    "  **Lasso Regression** is similar to Ridge Regression, except we have the absolute value of beta in our penalty term, the Î» term is a tuning parameter. When it is zero we get least squares, as it increases the term,the shrinkage penalty has more of an impact and the coefficients will equal 0.\n",
    "  \n",
    "  (iii) **Dimension Reduction** : In this approach, the features are transformed and then least squares model is fitted using the transformed features. p features are mapped into M-dimensional subspace where M < p, and there are M are different linear combinations of p. Hence it becomes computaionally feasible and simpler to estimate M+1 coefficients rather than p+1 coefficients. **Principal Components Regression(PCR)** and **Partial Least Squares(PLS)** are common methods for dimension reduction.\n",
    "  \n",
    "\n",
    "* Complete the first 3 exercises from Chapter 3 in Python\n",
    "\n",
    "    **Exercise Answer 1**\n",
    "\n",
    "    Null hypotheses is that there is no relation between product sales and advertising done through TV, radio or newspaper. However, the p-values of TV and advertising budgets each are < 0.0001, thus we can reject null hypotheses and conclude that TV and radio have positive impact on sales. p-value of newspaper advertising is 0.8599 which means it has no effect on the sales.\n",
    "\n",
    "    **Exercise Answer 2**\n",
    "\n",
    "    KNN classifier : It is a non-parametric method which gives qualitative output that matches the with the label of the majority of the K neareast neighbors.\n",
    "\n",
    "    KNN regression : It is a non-parametric method which gives quantitative output that gives average value of the output of the K neareast neighbors.\n",
    "\n",
    "    **Exercise Answer 3**\n",
    "\n",
    "    (a) Y(males) = 50 + 20(GPA) + 0.07(IQ) + 35(0) + 0.01(GPA)(IQ) + (-10)(GPA)(0)\n",
    "\n",
    "    Y(males) = 50 + 20(GPA) + 0.07(IQ) + 0.01(GPA)(IQ)\n",
    "\n",
    "    Y(females) = 50 + 20(GPA) + 0.07(IQ) + 35(1) + 0.01(GPA)(IQ) + (-10)(GPA)(1)\n",
    "\n",
    "    Y(females) = 85 + 10(GPA) + 0.07(IQ) + 0.01(GPA)(IQ)\n",
    "\n",
    "    If we take the values of GPA from 1 to 5, we can see that for lower values of GPA (1,2,3), salary of females is higher than males. But when GPA is 4 or 5, then salary of males is more than that of females. This means at certain GPA, salary of males start increasing. We can find that cut-off GPA :\n",
    "\n",
    "    50 + 20GPA >= 85 + 10GPA\n",
    "\n",
    "    GPA >= 3.5\n",
    "\n",
    "    So when GPA is greater than 3.5, males earn higher than females. So answer iii is correct.\n",
    "\n",
    "    (b)Predict the salary of a female with IQ of 110 and a GPA of 4.0. (Done at the end)\n",
    "\n",
    "    (c)Not sure, but we will still need to check p-value and if it is significant (< 0.5), then we can say there is an effect of interaction term.\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "\n",
    "    Project github repo is set up at https://github.com/Anushi/hospital_readmissions.git\n",
    "\n",
    "    Check out https://github.com/Anushi/hospital_readmissions/blob/master/hospital_readmissions.ipynb file for project rest of the answers\n",
    "    \n",
    "\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise Answer 3, part b\n",
    "# Predict the salary of a female with IQ of 110 and a GPA of 4.0.\n",
    "# Formula Y = 85 + 10(GPA) + 0.07(IQ) + 0.01(GPA)(IQ)\n",
    "85 + 10*4 + 0.07*110 + 0.01*4*110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
