{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "    * Explain sections 6-13 in your own words\n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "* Complete the first 3 exercises from Chapter 3 in Python\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as ml\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "Linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. The least square approach minimises the RSS. The whole point of linear regression is to minimise the sum of squared errors.\n",
    "\n",
    "A residual sum of squares (RSS) is a statistical technique used to measure the amount of variance in a data set that is not explained by the regression model. The residual sum of squares is a measure of the amount of error remaining between the regression function and the data set. A smaller residual sum of squares figure represents a regression function which explains a greater amount of the data. \n",
    "\n",
    "**Assessing the accuracy of the coefficient estimates\n",
    "\n",
    "Null Hypothesis - there is no relationship between X and Y\n",
    "Alternate Hypothesis - there is some relationship between X and Y\n",
    "\n",
    "To test Null Hypothesis, we need to determine whether our estimate of the coefficient of X is sufficiently far enough from zero that we can be confident that the actual value is non-zero. We compute the t-statistic which measures the number of standard deviations the estimated coefficient is away from zero.\n",
    "\n",
    "A small **p-value** indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response. Hence if we see a small p-value, we can infer that there is an association between the predictor and the response.\n",
    "\n",
    "The **coefficient of determination**, r2, is useful because it gives the proportion of the variance (fluctuation) of one variable that is predictable from the other variable. It is a measure that allows us to determine how certain one can be in making predictions from a certain model/graph. The coefficient of determination is such that 0 <  r2< 1,  and denotes the strength of the linear association between x and y. The coefficient of determination represents the percent of the data that is the closest to the line of best fit.  For example, if r = 0.922, then r2 = 0.850, which means that 85% of the total variation in y can be explained by the linear relationship between x and y (as described by the regression equation).  The other 15% of the total variation in y remains unexplained.\n",
    "\n",
    "> Good description. Maybe add a few lines with fewer technical terms as well, like, 'Regression to the mean is an inescapable fact of life. Your children can be expected to be less exceptional (for better or worse) than you are. Your score on a final exam in a course can be expected to be less good (or bad) than your score on the midterm exam, relative to the rest of the class. A baseball player's batting average in the second half of the season can be expected to be closer to the mean (for all players) than his batting average in the first half of the season. And so on. The key word here is \"expected.\" This does not mean it's certain that regression to the mean will occur, but that's the way to bet!\n",
    "> The intuitive explanation for the regression effect is simple: the thing we are trying to predict usually consists of a predictable component (\"signal\") and a statistically independent unpredictable component (\"noise\"). The best we can hope to do is to predict (only) that part of the variability which is due to the signal. Hence our forecasts will tend to exhibit less variability than the actual values, which implies a regression to the mean.' from http://people.duke.edu/~rnau/regintro.htm\n",
    "\n",
    "\n",
    "#### Useful things to know about machine learning\n",
    "\n",
    "The paper majorly focuses on ideas surrounding Machine Learning from Classification models' point of view. The paper was too complicated for my understanding. From what I could understand, they key take aways from the paper and those which we have covered are:\n",
    "\n",
    "- No matter how mych data we have, it is very unlikely that we will see those exact data in the test or production environment. So, doing well on the training set is not a measure of success.\n",
    "- **Machine learning is not magic, it can't get something from nothing. What it does is get more from less.**\n",
    "- **Bias** and **Variance** using the dart example\n",
    " - Bias is a learner's tendancy to consistantly learn the same wrong thing\n",
    " - Variance is the tendancy to learn random things irrespective of the real signal.\n",
    "- Cross validaton and k-folds cross validation - Both can help combat overfitting.\n",
    "\n",
    "> Good explanation. Well written in your own words, this can be a tricky thing to do when you've only just learned some of the terms and you've done a very good job.\n",
    "\n",
    "The *Curse of dimensionality* was a term coined by Bellman in 1961 which refers tothe fact that many algorithms that work find in low dimensions become unmanageable when the input is high dimensional. One might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact the curse of dimensionality might over-weigh their benefits.\n",
    "\n",
    "The most important factor in making a machine leaning project a success or failure is the features used. If you have many independant features that each correlate well with teh class, learning is easy. Often the raw data is not in a form that the learner can fully utilize, but you can construct features from it that are. This is typically where most of the effort in a machine learning project goes.\n",
    "\n",
    "Machine learning is not one-shot process of cleaning a dataset and running a learner, but rather an iterative process of running, analyzing the results, modifying the data/learner and repeating.\n",
    "\n",
    "A dumb algorithm with lots and lots of data beats a clever one with modest amount of data. Machine learning is all about letting data do the heavy lifting.\n",
    "\n",
    "> That's a good explanation, what about some of the other sections like Causality vs causation? And Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine Learning\n",
    "#### Chapter 3 - Question 1 to 3\n",
    "\n",
    "Question 1: \n",
    "Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/ISLR_Data/Advertising.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   570.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 25 Apr 2016</td> <th>  Prob (F-statistic):</th> <td>1.58e-96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:38:37</td>     <th>  Log-Likelihood:    </th> <td> -386.18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   780.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   196</td>      <th>  BIC:               </th> <td>   793.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    2.9389</td> <td>    0.312</td> <td>    9.422</td> <td> 0.000</td> <td>    2.324     3.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TV</th>        <td>    0.0458</td> <td>    0.001</td> <td>   32.809</td> <td> 0.000</td> <td>    0.043     0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Radio</th>     <td>    0.1885</td> <td>    0.009</td> <td>   21.893</td> <td> 0.000</td> <td>    0.172     0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Newspaper</th> <td>   -0.0010</td> <td>    0.006</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.013     0.011</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>60.414</td> <th>  Durbin-Watson:     </th> <td>   2.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 151.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.327</td> <th>  Prob(JB):          </th> <td>1.44e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.332</td> <th>  Cond. No.          </th> <td>    454.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Sales   R-squared:                       0.897\n",
       "Model:                            OLS   Adj. R-squared:                  0.896\n",
       "Method:                 Least Squares   F-statistic:                     570.3\n",
       "Date:                Mon, 25 Apr 2016   Prob (F-statistic):           1.58e-96\n",
       "Time:                        12:38:37   Log-Likelihood:                -386.18\n",
       "No. Observations:                 200   AIC:                             780.4\n",
       "Df Residuals:                     196   BIC:                             793.6\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      2.9389      0.312      9.422      0.000         2.324     3.554\n",
       "TV             0.0458      0.001     32.809      0.000         0.043     0.049\n",
       "Radio          0.1885      0.009     21.893      0.000         0.172     0.206\n",
       "Newspaper     -0.0010      0.006     -0.177      0.860        -0.013     0.011\n",
       "==============================================================================\n",
       "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
       "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
       "Kurtosis:                       6.332   Cond. No.                         454.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statsmodel API\n",
    "#model = smf.ols(formula='Sales ~ TV', data=df[:100])\n",
    "model = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=df)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1:\n",
    "Null Hypotheses for TV: TV and Sales do not have any relationship when we include radio and newspaper\n",
    "Null Hypotheses for Radio: Radio and Sales do not have any relationship when we include TV and newspaper\n",
    "Null Hypotheses for Newspaper: Newspaper and Sales do not have any relationship when we include radio and TV\n",
    "\n",
    "coef: predicted coefficients beta0 and beta1\n",
    "std err: standard error\n",
    "t: chef/std err. It tells us how standard deviations far off is the coefficient\n",
    "p value: We use p-values to determine statistical significance in a hypothesis test. Since the p-value of TV and Radio is less than 0.05, the null hypothesis does not hold true for them. In case of Newspaper, Null hypothesis does hold true as the p-value is greater than 0.05.\n",
    "\n",
    "> That's right, so just add another line to explain which values would actually be included. In this case we would not include Newspaper because the p-value was < 0.05 so we would not reject the null hypothesis. We would include the intercept, tv and radio features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "Carefully explain the differences between the KNN classifier and KNN regression methods.\n",
    "\n",
    "Answer 2:\n",
    "KNN classifier gives a classification output for Y. KNN regression predicts the quantitative value of Y.\n",
    "\n",
    "> How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3(a)(i)\n",
    "Y = 50 + 20*x1 + 0.07*x2 + 35*x3 + 0.01*x4 + -10*x5\n",
    "where,\n",
    "x1 = GPA\n",
    "x2 = IQ\n",
    "x3 = Gender\n",
    "x4 = Interaction between GPA and IQ\n",
    "x5 = Interaction between GPA and Gender\n",
    "Or,\n",
    "Y = 50 + 20*x1 + 0.07*x2 + 35*x3 + 0.01*(x1*x2) + -10*(x1*x3)\n",
    "\n",
    "So,\n",
    "For Male (x3=0)  : Y = 50 + 20*x1 + 0.07*x2 + 0.01*(x1*x2)\n",
    "For Female (x3=1): Y = 50 + 20*x1 + 0.07*x2 + 35 + 0.01*(x1*x2) + -10*(x1)\n",
    "\n",
    "Due to the negative factor at the end, the salary would depend on the GPA.\n",
    "\n",
    "Answer 3(a)(ii)\n",
    "Cannot tell whether male or female earn more on average than males.\n",
    "\n",
    "Answer 3(a)(iii)\n",
    "On high values of GPA, male earn more than females due to the negative factor -10*(x1)\n",
    "\n",
    "Answer 3(a)(iv)\n",
    "On high values of GPA, male earn more than females due to the negative factor -10*(x1)\n",
    "\n",
    "> So (iii) is the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.1\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "#Answer 3(b)\n",
    "y = 50 + 20*4.0 + 0.07*110 + 35*1 + 0.01*(4.0*110) + -10*(4.0*1)\n",
    "print y\n",
    "print 'Correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3(c)\n",
    "The coefficient of GPA/IQ interaction is very small. That does not mean that the interaction effect on the regression will be negligible. The p-value of the coefficient will decide the effect of the variable.\n",
    "> Correct, well written explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 ways we can select what features to use in a model\n",
    "\n",
    "A standard linear model is commonly used to describe the relationship between a response Y and a set of parameters X1, X2 .. Xp. One usually fits this model using least squares.\n",
    "\n",
    "Alternate fitting procedures can result in better prediction accuracy and model interpretability. If the number of observations n is not much larger than the number of variables p, then least squares method can produce a lot of variablity, resulting in over-fitting and poor predictions on future observations noy used in model training.\n",
    "\n",
    "There might also be the scenario that the variables are not associated with the response. Including such variables adds to the complexity of the model.\n",
    "\n",
    "Shrinkage, or regularization, is an approach that involves all the p predictors. However the coefficients are shrunken towards zero and this has an effect of reducing variance.\n",
    "\n",
    "1. Ridge Regression\n",
    "2. Lasso\n",
    "3. Elastic Net\n",
    "\n",
    "> Good description, well done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
