{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account. Done.\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "\n",
    "Say you have a two column table of data from your local car dealership. The data has been segmented by day, (each row represents a different day). The first column tells us the number of salespeople working, which we can refer to as \"x\", and the second column tells us total sales for that day, which we can refer to as \"y\".\n",
    "\n",
    "On its own, this table provides a simple historical summary for the local car dealership. We might be able to observe some relationship between the two columns that support our expectations, like that there tends to be more sales when more salespeople are working, however we can't consistently describe this by looking at each row individually. However, we can use a technique called linear regression to summarise the relationship between salespeople working and sales, or 'x' and 'y', for the table as a whole.\n",
    "\n",
    "Linear regression models describe the relationship between a dependent variable, in this case sales, and an independent variable, in this case salespeople working.\n",
    "\n",
    "If you visualise the table on a scatter plot, where the horizontal plane ('x') represents salespeople working and the vertical plane ('y') represents sales, a simple linear represents a 'best fit' line through the data points.\n",
    "\n",
    "This line and its equation is the model itself, meaning the dealership owner can estimate an expected volume of sales on any day for a given number of salespeople simply by observing where x intersects with the line on the y-axis.\n",
    "\n",
    "However, the linear regression models need to be assessed as to whether or not they are useful. One of these assessments is calculating the r-squared value of the model. R-squared essentially describes how well the model fits the data in comparison to just having a straight line through the average 'y' value. This is number between 0 and 1 where a result of 1 means the model perfectly fits all the data points. The p-value also similarly tests the model against a 'dummy model' called a null hypothesis, where the co-efficients (or slope of the line) has no value.\n",
    "\n",
    "\n",
    "\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). Done.\n",
    "    * What have we covered so far from this paper? \n",
    "    \n",
    "        *Logistic Regression\n",
    "        *Cross-Validation\n",
    "        *Bias and variance, bias-variance trade-off\n",
    "        *Regularisation\n",
    "        *Dimensionality Reduction\n",
    "    \n",
    "    * Explain sections 6-13 in your own words\n",
    "    \n",
    "        6. Intuition: High dimensionality is an issue because it makes the data too noisy for a lot of modelling techniques and also it stops being intuitively represented by bell curves and other simple distributions.\n",
    "        7. Theoretical Guarantees are Not What They Seem: theoretical guarantees used to aid algorithm and model design, but are to be approached very cautiously in practice as they are unreliable.\n",
    "        8. Feature Engineering is the Key: not as much time as expected is spent on actual machine learning, mostis dedicated to preparing data and engineering features. This takes domain expertise versus general application of learning algorithms\n",
    "        9. More Data Beats a Cleverer Algorithm: If you aren't getting the accuracy of results you want, getting more data is better than increasing the complexity of the algorithm. The catch is time and resources. Clever algorithms that get he most from data and resources do pay off, but it is important the efficiency is continued through human elements of machine learning when implementing etc.\n",
    "        10. Learn Many Models, Not One: creating model ensembles is now standard practice. Bagging is where you train a model on randomised variations of the dataset and combine results by voting. Boosting is where each training model iteration focuses on the weak points of the prior models.\n",
    "        11. Simplicity does not imply Accuracy: simple models may not necassarily be preferable just for being simpler. Models tend to need to reflect the complexity of their hypothesis space. However, the pursuit of simplicity is still valuable.\n",
    "        12. Representable does not imply Learnable: just because you can write an equation doesn't mean that it the model is learnable.\n",
    "        13. Correlation does not imply Causation: experimental data where effects are controllable are highly valued, but for observational data remember that you are probably not truly measuring cause and effect\n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "    1. forward selection - we start off with null hypothesis and add linear regressions starting with the lowest RSS and then keep adding until a stopping rule is satisfied\n",
    "    2. backward selection - we start with all features in the model and then remove them starting from largest p-value until a stopping rule is satisfied\n",
    "    3. mixed selection - combination of both. start with forward selection but reevaluate all features at the introduction of each additional feature and remove any that migrates above a certain threshold under the changed model\n",
    "\n",
    "* Complete the first 3 exercises from Chapter 3 in Python. \n",
    "    1. The p-values given in table 3.4 correspond to a null hypothesis that media channel spend has zero relationship to sales, or simply a co-efficient of 0 in the multiple regression model. From the resultant p-value one can conclude that there is an observable relationship between sales and TV and Radio spend, but not for newspapers. Although if analysed in isolation a relationship might be evident, once you incorporate TV and Radio spend into the regression model there is no observable relationship between sales and newspaper spend. That is to say, the effect of spending an extra $1,000 on newspaper advertising will no observable difference than $10,000 or $10.\n",
    "    \n",
    "    2. The KNN classifier method estimates the probability for a given test observation is of a given class based on the classification of its K nearest datapoints. THE KNN regression method identifies the closest K datapoints to a test observation and then estimates the value by averaging all the training responses from K observed datapoints.\n",
    "    \n",
    "    3. a) iii is true because although females earn more than males when GPAs are lower, it is not true once GPAs are higher (around 4) therefore, the only consistently true statement is that males earn more an average provided the GPA is high enough. This is occuring becasue of the negative co-efficient for interaction between gender and GPA.\n",
    "    b) $137,100\n",
    "    c) False. The coefficient may be small but it is interacting with large numbers which can have a noticeable effect on the predictions. Say I have an IQ of 100 and GPA of 3, this would represent an additional $3,000 per year or $60 per week, which is not insignificant given the context of the dataset.\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "\n",
    "This is saved as an image in the directory\n",
    "\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "This is addressed in the exploratory worksheet\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.1\n"
     ]
    }
   ],
   "source": [
    "#calculating Q3 pay\n",
    "\n",
    "GPA = 4\n",
    "IQ = 110\n",
    "Gender = 1\n",
    "Interaction1 = GPA * IQ\n",
    "Interaction2 = GPA * Gender\n",
    "\n",
    "pay = 50 + (20*GPA) + (0.07*IQ) + (35*Gender) + (0.01*Interaction1) + (-10*Interaction2)\n",
    "\n",
    "print pay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
