{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "    - DONE - \n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "    - DONE - \n",
    "**Linear Regression**\n",
    "1) Linear Regression is a mathematical/statistic technique for modelling the relationship between a dependent variable (often denoted as Y) and a single or multiple independent variables (often denoted as Xs).\n",
    "2) The simpliest form of Linear Regression is as follows: Y = a + bX + E where Y is the variable we are trying to predict (the dependent variable), X is the varaible we are trying to predict Y with, a is the intercept of the straight line we are fitting, b is the slope and E is the residues or error term of the line fitting. In this case, we are trying to form a relationship between Xs and Ys. The a/b denote the \"characteristics\" of the straight line relationship we are fitten between X and Y. An example of simple regression could be we are trying to find the relationship or predict seawater temperature (Y) by looking at current air temerpature (X).\n",
    "3) Multiple linear regression is an extension of the above but we introduce many Xs. This takes the form: Y = a + b1X1 + b2X2 + ... + bnXn + E. Any example of multiple regression could be we are trying to find the relationship or predict seawater temperature (Y) by looking at a number of factors such as current air temerpature (X1), amount of sunlight (X2), wind speed (X3) etc...\n",
    "4) To arrive at the values of a(s) and b(s), linear regression uses a \"least squares\" approach. In short, if we were to use simple regression as an example, the technique attempts to draw a straight line through the scatterplot (where x-axis is the predictor variable and y-axis is the predicted variable) and determines the \"best\" fit by minimising the square of the difference of the observed value and the fitted (drawn straight line) value. The square is used to cater for negative numbers.\n",
    "**p-value**\n",
    "1) In statistics p-value is used to determine the significance of a statistical test. In relation to linear regression, the hypothesis test we are performing is whether the coefficient (b) of the predictor variable is zero. A low p-value indicates that you can reject the null hypothesis. This means the predictor variable (X) used for fitting is likely to add value to the linear regression model because changes in Y is likely due to X.\n",
    "2) The \"level\" at which you determine whether a p-value is low or not can be disputed. 0.05 is the norm for most statistical tests. Tests requiring more vigor demand significance at a 0.01 level or less, while some data investigation work can accept 0.1 if it is used for exploratory purposes only.\n",
    "**R-squared**\n",
    "1) R-square is a descriptive statistic used in regression to help the modeller gauge the \"fit\" of the model (but it is not the only one!)\n",
    "2) R-square describes the amount of variance the fitted model can account for. The higher the r-square, the better fit the model. R-square can take values between 0 and 1. While a higher r-square is desired, evaluation of the model should be done not just on r-square.\n",
    "\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "1) Some modelling techniques which form the basis of the \"Representation\" phase of machine learning.\n",
    "2) Understand the concept and purpose of building a model - to generalise relationships from variables to a predicted observation so we can make predictions.\n",
    "3) When building a model, we need to be careful not to overfit. Training/hold-out data can help reduct the instances of overfitting.\n",
    "4) We need to understand variance and bias and they role they play in determining what is an appropriate model for use.\n",
    "    * Explain sections 6-13 in your own words\n",
    "*Section 6*\n",
    "     - Human processing struggles with higher dimension thinking (i.e. thinking that involves more than 3 dimensions, in our case, e.g. a dataset with 4 or more variables and we try to form relationships within/between them in our heads).\n",
    "     - For example, a Scatterplot can show us nicely how two variables are related to each other. A 3D scatterplot can help us visually depict 3 dimensions and how 3 variables interact with each other. Clever grouping and colour in a 3D scatterplot can probably even depict 4 or 5 dimensions.\n",
    "     - However, anymore relationships are no longer \"easy\" to spot and can even be incorrect due to how variables can interact with each other. Interaction effects and correlations need ot be taken into account when doing data analyses on a dataset with many variables.\n",
    "\n",
    "*Section 7*\n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning\n",
    "    - UP TO PAGE 240 - 16 more pages to go - \n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "\n",
    "* Complete the first 3 exercises from Chapter 3 in Python\n",
    "**Question 1**\n",
    "The null hypothesis of the p-values is that the coefficient of the predictor variables are zero. Testing for the null hypothesis hopes to reject this (i.e. rejecting it means the variable coefficient is significant and can add value to our model).\n",
    "\n",
    "In this example, TV and Radio are both considered significant. That is because the p-value are less than 0.05, meaning we have evidence to reject the null hypothesis of zero coefficients. In the context of the regression analyses, it shows that spend on TV and Radio can potentially increase sales.\n",
    "\n",
    "Newspaper however tells a different story. The p-value is greater than 0.05 meaning we do not have enough evidence to reject the null hypothesis. The variable is not significant (i.e. there is no evidence to suggest that spending more on newspaper would increase sales). Inspecting the coefficient of the newspaper variable is interesting, as the negative sign indiciated that the more you spend on newspaper, the less sales you'll be expected to recieve!!!\n",
    "\n",
    "**Question 2**\n",
    "KNN Classificiation takes a point, and computes the distance between it and all other training points. Based on the value of K, the point takes class of the majority of the K nearest points.\n",
    "\n",
    "KNN regression you are trying to predict Ys. Compute the K nearest neighbours and average those Ys (as opposed to taking the majority) to recieve your \"predicted\" Y value. K-NN methods are relatively good for interpolation but rather poor for exterpolation.\n",
    "\n",
    "**Question 3** - NOT DONE YET!! - \n",
    "Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get βˆ0 = 50, βˆ1 = 2 0 , βˆ 2 = 0 . 0 7 , βˆ 3 = 3 5 , βˆ 4 = 0 . 0 1 , βˆ 5 = − 1 0 .\n",
    "(a) Which answer is correct, and why?\n",
    "i. ForafixedvalueofIQandGPA,malesearnmoreonaverage than females.\n",
    "ii. For a fixed value of IQ and GPA, females earn more on average than males.\n",
    "iii. ForafixedvalueofIQandGPA,malesearnmoreonaverage than females provided that the GPA is high enough.\n",
    "iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.\n",
    "(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.\n",
    "(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
