{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "    * Done (with flavia_rossler@yahoo.com.au)\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "    * Linear regression consists of finding the best-fitting straight line through the points. It is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. It assumes that there is a linear relationship between a dependent variable and one or more independent variables.\n",
    "    * The simplest form with one dependent and one independent variable is defined by the formula y = c + b*x, where y = estimated dependent, c = constant, b = regression coefficients, and x = independent variable. The objective is to minimise the residual sum of squares, which are the difference between the observed value and the predicted value.\n",
    "    * Overall process includes the following steps:\n",
    "        * Visualise and understand the data \n",
    "        * Choose the independent variables (regularization) \n",
    "        * Fit the model \n",
    "        * Evaluate and optimise the model \n",
    "    * The **P-value** is the level of marginal significance within a statistical hypothesis test, representing the probability of the occurrence of a given event. The p-value is used as an alternative to rejection points to provide the smallest level of significance at which the null hypothesis would be rejected. The smaller the p-value, the stronger the evidence is in favor of the alternative hypothesis. The p-value is a number between 0 and 1 and interpreted in the following way:\n",
    "        * A small p-value (typically â‰¤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis. If p-value is small, include it in the model.\n",
    "        * A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.\n",
    "        * p-values very close to the cutoff (0.05) are considered to be marginal (could go either way). Always report the p-value so your readers can draw their own conclusions.\n",
    "    * **R-squared** is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. R-squared is the percentage of the response variable variation that is explained by a linear model (*R-squared = Explained variation / Total variation*) and it is always between 0 and 100%:\n",
    "        * 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "        * 100% indicates that the model explains all the variability of the response data around its mean\n",
    "        \n",
    "> Good explanation and description of terms. One thing I would suggest if you are explaining it to other people would be to explain it with an example, like explaining the house price based on area, bathrooms, age, etc. But really well written.\n",
    "\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "        * Representation (K-means, Logistic Regression)\n",
    "        * Evaluation (Accuracy/Error rate, Precision and recall, Squared error)\n",
    "        * The problem of overfitting and the ways to decompose generalization error into bias and viariance\n",
    "        * Cross-validation\n",
    "        \n",
    "> Yes, if the connection with what we have done and the topics listed in the paper aren't clear please let me know.\n",
    "\n",
    "    * Explain sections 6-13 in your own words\n",
    "        * *Dimensionality* is a big problem in machine learning as many algorithms won't work well with high-dimensional inputs. \n",
    "        * The complexity of correct generalisation increases as the number of features (dimensonality) of the examples grows because a fixed sized training set covers a diminishing fraction of the input space \n",
    "        * High dimensionality also cause the similarity-based reasoning to break down and decreases the power of our intuition as it comes from a three dimensional world\n",
    "        * *Blessing of non-uniformity*: when examples are not spread uniformily throughout the instance space but are concentrated on a near lower dimensional manifold \n",
    "        * Given a large enough training set with high probability the learner will either return a hypothesis that generalizes well or be unable to fina a constant hypothesis. If the true classifier is in the hypothesis space, the probability that the learner outputs a bad classifier decreases with the training set size. \n",
    "        * The success of a machine learning project is highly dependable on the features used. If there are many independent variables that each correlate well with the class, learning is easy. However, if the class is a very complex function of the features, the class might not be predictable at all. For this reason, the data gathering, manipulation, cleansing, pre-processing is a very time-consuming step o machine learning projects.\n",
    "        * Machine learning is an interative process of running the learner, analysing the results, modifying the data and/or the learner, and repeting.\n",
    "        * There are two ways to deal with badly performing learners: design a better learning algorithm or gather more data (more examples).\n",
    "        * Whilst more data can support better learning, it can lead to scalability due to limited time and memory. For that reason, as a rule, it pays to try the simples learners first.\n",
    "        * Learners can be divided into two groups: Parametric (the ones where representation has a fixed size - like linear classifiers) and non-parametric (those whose representation can grow with the data - like decision trees). Non-parametric problems are usually better learned from more complex algorithms.\n",
    "        * Whilst complex learners can lead to better accuracy, they are not always easy to understand and to be explained. The human gained insights are a very important factor of machine learning. So, sometimes the decision of the learner favors learners that produce human-understandable outputs. \n",
    "        * The choice of learner varies from application to application. This insight lead to the trial of many variation of many learners and the fact that the combination of several learners could result in better outcomes. This is aka *model ensembles*.  \n",
    "            * *Bagging* is the technique to generate random variations of the training set by resampling, learn a classifier on each, and combine the results by voting. This greatly reduces variance while only slightly increases bias. \n",
    "            * In *Boosting*, training examples have weights, and these are vaired so that each new classifier focuses on the examples the previous ones tended to get wrong.  \n",
    "            * In *Stacking*, the outputs of individual classifiers become the inputs of a higher-level learner that figures out how best to combine them.' \n",
    "        * In model ensembles, predictions on new examples are made by averaging the individual predictions of all classifiers in the hypothesis space, weighted by how well the classifiers explain the training data and how much we believe in them a priori. Ensembles changethe hypothesis space, and can take a wide variety of forms. Bayesian model averaging assigns weights to the hypotheses in the originial space according to a fixed formula. Model ensembles are a key part of the machine learning toolkit.  \n",
    "        * Simpler hypotheses should be preferred because simplicity is a virtue in its own right, not because of a hypothetical connection with accuracy.\n",
    "        * Representable does not imply learnable. Therefore the key question is not *Can it be represented?*, to which the answer is often trivial, but *Can it be learned?*. And it pays to try different learners (and possibly combine them). \n",
    "        * Correlation does not imply causation. \n",
    "        \n",
    "> Really good points. I think you've done a great job of extracting the main takeaways. There's some very interesting ideas in sections 6-13 that we will cover in more detail in the reamining parts of the course. And all of these ideas are useful to keep front of mind when working on your project. All these ideas are important in machine learning and I think you've explained them very well in the above.\n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "    * *Subset Selection* - This approach involves identifying a subset of the *p* predictors that we believe to be related ot the response. We then fit a model using least squares on the reduced set of variables\n",
    "    * *Shrinkage* - This approach involves fitting a model involving all *p* predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This *regularization* has the effect of reducing variance. \n",
    "    * *Dimension Reduction* - This approach involves *projecting* the *p* predictors into a *M*-dimensional subspace, where *M* < *p*. This is achieved by computing *M* different *linear combinations*, or *projections*, of the variables. Then these *M* projections are used as predictors to fit a linear regression model by least squares.\n",
    "\n",
    "> Well articulated.\n",
    "    \n",
    "* Complete the first 3 exercises from Chapter 3 in Python\n",
    "    * Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio and newspaper, rather than in terms of the coefficients of the linear model.  \n",
    "        * Radio appears to be the channel that offers the best return of investiment, where an additional spend of \\$1,000 in advertising via this channel would result in an increase of approximaley 189 units in sales. Whilst TV advertising spend also predicts a good return, it's not as high as radio (for each \\$1,000 additional spend in TV, the sales are expected to increase by 46 points). Both TV and Radio have a near to zero p-value what indicates that there is a relationship between TV and Radio and the outcome Sales. Newspaper advertising, however, does not show a strong relationship with sales, indicating that it's not really worthy spending marketing funds in this particular media (high p-value indicates that there is no enough evidence to reject the null hypothesis and that the variable newspaper is not statistically significant).\n",
    "        \n",
    "> Great explanation of variables. I like how you expressed it in terms of the original question the business would have been asking. Good work!\n",
    "        \n",
    "    * Carefully explain the differences between KNN classifier and KNN regression methods.\n",
    "        * KNN classifier is a method to be used to predict qualitative responses when computing Bayes classifier is impossible. It attempts to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. \n",
    "        * KNN regression method is a non-parametrhic approach to perform regression. Given a value for K and a prediction point x0, KNN regression first identifies the K training observations that are closes to x0, represented by N0. It then estimates f(x0) using the average of all the training responses in N0.\n",
    "        \n",
    "> CORRECT\n",
    "\n",
    "    * Suppose we have a data set with five predictors, *X1* = GPA, *X2* = IQ, *X3* = Gender (1 for Female and 0 for Male), *X4* = Interaction between GPA and IQ, and *X5* = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars) Suppose we use least squares to fit the model, and get *B0* = 50, *B1*  = 20, *B2* = 0.07, *B3* = 35, *B4* = 0.01, *B5* = -10.\n",
    "        * a) Which answer is correct and why?\n",
    "           * i. For a fixed value of IQ and GPA, males earn more on average than females.\n",
    "           * ii. For a fixed value of IQ and GPA, females earn more on average than males.\n",
    "           * **iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.** (correct answer - logic in code below)\n",
    "           * iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.\n",
    "           \n",
    "> CORRECT, great reasoning. \n",
    "           \n",
    "        * b) Predict the salary of a female with IQ of 110 and GPA of 4.0. Answer: **137.1**\n",
    "        \n",
    "> CORRECT\n",
    "        \n",
    "        * c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n",
    "            * The answer is false as the coeficient does not mean significance. p-value represents significance while coefficients represent the mean change in the response variable for one unit of change in the predictor variable while holding other predictors in the model constant.  \n",
    "            \n",
    "> CORRECT and well explained\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "> Excellent homework submission Flavia. Well written and explained answers. Communication can be complicated for these ideas that you've just learned, but you've done a great job in getting the ideas in ways a non-technical person could understand.\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.1\n"
     ]
    }
   ],
   "source": [
    "## X1 = GPA, X2 = IQ, X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Gender\n",
    "## B0 = 50, B1 = 20, B2 = 0.07, B3 = 35, B4 = 0.01, B5 = -10.\n",
    "def salary (GPA, IQ, Gender):\n",
    "    value = 50 + (GPA * 20) + (IQ * 0.07) + (Gender * 35) + (GPA * IQ * 0.01) - (GPA * Gender * 10)\n",
    "    print value\n",
    "    return\n",
    "\n",
    "## Predict the salary of a female with IQ of 110 and GPA of 4.0\n",
    "print salary(4.0,110,1)\n",
    "print \"CORRECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.4\n",
      "95.4\n",
      "184.6\n",
      "239.6\n"
     ]
    }
   ],
   "source": [
    "##i. For a fixed value of IQ and GPA, males earn more on average than females.\n",
    "##ii. For a fixed value of IQ and GPA, females earn more on average than males.\n",
    "##iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.\n",
    "##iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.\n",
    "\n",
    "\n",
    "\n",
    "#test IQ and GPA fix for female - low GPA\n",
    "salary(2.0,60,1)\n",
    "\n",
    "#test IQ and GPA fix for male - low GPA\n",
    "salary(2.0,60,0)\n",
    "\n",
    "#test IQ and GPA fix for female - high GPA\n",
    "salary(9.0,60,1)\n",
    "\n",
    "#test IQ and GPA fix for male - high GPA\n",
    "salary(9.0,60,0)\n",
    "\n",
    "\n",
    "## correct answer: iii"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
